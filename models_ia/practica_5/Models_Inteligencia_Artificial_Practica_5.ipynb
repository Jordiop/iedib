{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificació de text segons l'autor\n",
    "  Construïu dos models de diferents autors: Josep Carner i Miquel dels Sants Oliver. Per això, podeu usar les seves obres disponibles al Projecte Gutenberg. Per exemple, de Carner preneu-ne la traducció dels contes de Mark Twain. Després, classificau frases en l'estil de cadascú que mostrin com el vostre model les identifica correctament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca_core_news_sm@ https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl#sha256=e214211aa8da91c24ebdc453c2aa5f54fac09f44e01e65bcbdd3b0a5cb94d809 (from -r requirements.txt (line 5))\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
      "Collecting annotated-types==0.7.0 (from -r requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: appnope==0.1.4 in /Users/jordiop/Desktop/iedib/venv2/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /Users/jordiop/Desktop/iedib/venv2/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (3.0.0)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.2.0 Requires-Python <3.13,>=3.6\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement blis==1.2.0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.8, 0.0.9.dev104, 0.0.10, 0.0.12, 0.0.13, 0.0.16, 0.1.0, 0.2.0.dev0, 0.2.0, 0.2.1, 0.2.2.dev0, 0.2.2, 0.2.3.dev0, 0.2.3.dev1, 0.2.3.dev2, 0.2.3.dev3, 0.2.3, 0.2.4, 0.3.1, 0.4.0.dev0, 0.4.0.dev1, 0.4.0, 0.4.1, 0.7.0, 0.7.1.dev0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.9.0, 0.9.1.dev0, 0.9.1.dev1, 0.9.1, 1.0.0a1, 1.0.0, 1.0.1, 1.0.2, 1.1.0a0, 1.1.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for blis==1.2.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maima_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m open_data\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maima_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m words, UnigramWordModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maima_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NaiveBayesLearner\n",
      "File \u001b[0;32m~/Desktop/iedib/models_ia/practica_5/aima_data/utils.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chain, combinations\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatistics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ______________________________________________________________________________\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Functions on Sequences and Iterables\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msequence\u001b[39m(iterable):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from aima_data.utils import open_data\n",
    "from aima_data.text import words, UnigramWordModel\n",
    "from aima_data.learning import NaiveBayesLearner\n",
    "\n",
    "def recognize_author(text, nbs):\n",
    "   return nbs(words(text.lower()))\n",
    "\n",
    "def create_unigram_model(file_path, n):\n",
    "   text = open_data(file_path).read()\n",
    "   wordseq = words(text)\n",
    "   return UnigramWordModel(wordseq, n)\n",
    "\n",
    "p_miquel = create_unigram_model('CA-Text/bolla_hostel.txt', 5)\n",
    "p_josep = create_unigram_model('CA-Text/tom_sawyer_adventures.txt', 5)\n",
    "\n",
    "nbs = NaiveBayesLearner({('Miquel', 1): p_miquel, ('Josep', 1): p_josep}, simple=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara tenim les dues frases següents:\n",
    "De Miquel dels Sants Oliver:\n",
    "\"La història és un diàleg entre el passat i el present, una constant revisió de criteris i valors.\"\n",
    "De Josep Carner:\n",
    "\"Les paraules són com ocells que volen de branca en branca fins a trobar el seu cant definitiu.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_author('entre el passat i el present, una constant revisió de criteris i valors.')\n",
    "recognize_author('ocells que volen de branca en branca fins a trobar el seu cant definitiu.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generació de text\n",
    "Construïu models n-gram amb n=1, n=3, n=5 i n=7 a partir d'un text triat per vosaltres. Pot ser una obra del Projecte Gutenberg o una notícia de premsa, per exemple. Observau com a mesura que augmenta el nombre de paraules que s'hi han tingut en compte la versemblança del text generat és més gran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from aima_data.text import NgramCharModel, words\n",
    "\n",
    "# generate text funcion\n",
    "def generate_text(model, n, ngram=5):\n",
    "    generate = []\n",
    "    seed = choice(list(model.dictionary.keys()))\n",
    "    generate.extend(seed)\n",
    "\n",
    "    for i in range(n - ngram):\n",
    "        last = tuple(generate[-(ngram - 1):])\n",
    "        next_chars = [j[ngram - 1] for j in model.dictionary.keys() if j[:ngram - 1] == last]\n",
    "\n",
    "        if not next_chars:\n",
    "            break\n",
    "\n",
    "        next_char = choice(next_chars)\n",
    "        generate.append(next_char)\n",
    "\n",
    "    return ' '.join(generate)\n",
    "\n",
    "bollaHostel = open_data('CA-Text/bolla_hostel.txt').read()\n",
    "wordseq = words(bollaHostel)\n",
    "\n",
    "for i in range (1, 6):\n",
    "    model = NgramCharModel(wordseq, i)\n",
    "    print(generate_text(model, 100))\n",
    "    print(f'Ngram: {i}\\n')\n",
    "    print({generate_text(model, 100)})\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anàlisi sintàctica\n",
    "Aplicau les eines d'alguna llibreria Python per analitzar sintàcticament l'oració següent: Tots els éssers humans neixen lliures i iguals en dignitat i drets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download ca_core_news_sm\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "es_nlp = spacy.load('es_core_news_sm')\n",
    "ca_nlp = spacy.load('ca_core_news_sm')\n",
    "\n",
    "es_sentence = 'Todos los seres humanos nacen libres e iguales en dignidad y derechos'\n",
    "ca_sentence = 'Tots els éssers humans neixen lliures i iguals en dignitat i drets'\n",
    "\n",
    "es_doc = es_nlp(es_sentence)\n",
    "ca_doc = ca_nlp(ca_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('- CATALÀ -')\n",
    "print('Noun Phrases:', [chunk.text for chunk in ca_doc.noun_chunks])\n",
    "print('Verbs:', [token.lemma_ for token in ca_doc if token.pos_ == 'VERB'])\n",
    "print('Subjects:', [token.text for token in ca_doc if token.dep_ == 'nsubj'])\n",
    "print('Adjectius:', [token.text for token in ca_doc if token.pos_ == 'ADJ'])\n",
    "print('Preposicions:', [token.text for token in ca_doc if token.pos_ == 'ADP'])\n",
    "print('Conjuncions:', [token.text for token in ca_doc if token.pos_ == 'CCONJ'])\n",
    "print('+ Informació:')\n",
    "for token in ca_doc:\n",
    "    print(f'Paraula: {token.text} | Lema: {token.lemma_} | Part del discurs: {token.pos_} | Dependència sintàctica: {token.dep_} | Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('- CASTELLANO -')\n",
    "print('Noun Phrases:', [chunk.text for chunk in es_doc.noun_chunks])\n",
    "print('Verbos:', [token.lemma_ for token in es_doc if token.pos_ == 'VERB'])\n",
    "print('Sujetos:', [token.text for token in es_doc if token.dep_ == 'nsubj'])\n",
    "print('Adjetivos:', [token.text for token in es_doc if token.pos_ == 'ADJ'])\n",
    "print('Preposiciones:', [token.text for token in es_doc if token.pos_ == 'ADP'])\n",
    "print('Conjunciones:', [token.text for token in es_doc if token.pos_ == 'CCONJ'])\n",
    "print('+ Información:')\n",
    "for token in es_doc:\n",
    "    print(f'Palabra: {token.text} | Lema: {token.lemma_} | Parte del discurso: {token.pos_} | Dependencia sintáctica: {token.dep_} | Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperació d'informació\n",
    "Carregau l'article de la wikipèdia sobre Europa (https://ca.wikipedia.org/wiki/Europa) en una llista de frases i recuperau-ne la informació següent, cercant les frases més semblants.\n",
    "\n",
    "Quan es va gestar el concepte d'Europa?\n",
    "Quina és l'espècie humana autòctona d'Europa?\n",
    "Quan es varen formar els estats actuals d'Europa?\n",
    "Quin clima té Europa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# fetch wikipedia content function\n",
    "def fetch_wikipedia_content(url):\n",
    "    response = get(url)\n",
    "    html = BeautifulSoup(response.text, 'html.parser')\n",
    "    content = ' '.join([p.text for p in html.find_all([\"p\", \"div\"]) if p.text])\n",
    "    return content\n",
    "\n",
    "# processor text with spacy function\n",
    "def process_text_with_spacy(content, model_name='ca_core_news_sm'):\n",
    "    nlp = spacy.load(model_name)\n",
    "    document = nlp(content)\n",
    "    sentences = [sentence.text for sentence in document.sents]\n",
    "    return sentences\n",
    "\n",
    "# best matching thing\n",
    "def find_best_matching_sentences(questions, sentences, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    questions_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "    sentences_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    best_sentences = []\n",
    "    for i, question in enumerate(questions):\n",
    "        scores = util.pytorch_cos_sim(questions_embeddings[i], sentences_embeddings)\n",
    "        best_sentence_idx = scores.argmax()\n",
    "        best_sentences.append(sentences[best_sentence_idx])\n",
    "    return best_sentences\n",
    "\n",
    "url = 'https://ca.wikipedia.org/wiki/Europa'\n",
    "content = fetch_wikipedia_content(url)\n",
    "sentences = process_text_with_spacy(content)\n",
    "\n",
    "questions = [\n",
    "    '¿Cuándo se gestó el concepto de Europa?',\n",
    "    '¿Cuál es la especie humana autóctona de Europa?',\n",
    "    '¿Cuándo se formaron los estados actuales de Europa?',\n",
    "    '¿Qué clima tiene Europa?'\n",
    "]\n",
    "\n",
    "best_sentences = find_best_matching_sentences(questions, sentences)\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    print(f'Question {i+1}: {question}\\nBest Sentence: {best_sentences[i]}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
