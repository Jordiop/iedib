{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificació de text segons l'autor\n",
    "  Construïu dos models de diferents autors: Josep Carner i Miquel dels Sants Oliver. Per això, podeu usar les seves obres disponibles al Projecte Gutenberg. Per exemple, de Carner preneu-ne la traducció dels contes de Mark Twain. Després, classificau frases en l'estil de cadascú que mostrin com el vostre model les identifica correctament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting blis==1.2.0\n",
      "  Downloading blis-1.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting numpy<3.0.0,>=1.19.0 (from blis==1.2.0)\n",
      "  Downloading numpy-2.2.3-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Downloading blis-1.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.3-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, blis\n",
      "Successfully installed blis-1.2.0 numpy-2.2.3\n",
      "Collecting ca_core_news_sm@ https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl#sha256=e214211aa8da91c24ebdc453c2aa5f54fac09f44e01e65bcbdd3b0a5cb94d809 (from -r requirements.txt (line 6))\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting es_core_news_sm@ https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (from -r requirements.txt (line 17))\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types==0.7.0 (from -r requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: appnope==0.1.4 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.0.0)\n",
      "Collecting beautifulsoup4==4.13.3 (from -r requirements.txt (line 4))\n",
      "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bs4==0.0.2 (from -r requirements.txt (line 5))\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting catalogue==2.0.10 (from -r requirements.txt (line 7))\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting certifi==2025.1.31 (from -r requirements.txt (line 8))\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting charset-normalizer==3.4.1 (from -r requirements.txt (line 9))\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting click==8.1.8 (from -r requirements.txt (line 10))\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting cloudpathlib==0.20.0 (from -r requirements.txt (line 11))\n",
      "  Using cached cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: comm==0.2.2 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.2.2)\n",
      "Collecting confection==0.1.5 (from -r requirements.txt (line 13))\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cymem==2.0.11 (from -r requirements.txt (line 14))\n",
      "  Downloading cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: debugpy==1.8.12 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (1.8.12)\n",
      "Collecting decorator==5.1.1 (from -r requirements.txt (line 16))\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (1.2.2)\n",
      "Requirement already satisfied: executing==2.2.0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (2.2.0)\n",
      "Collecting filelock==3.17.0 (from -r requirements.txt (line 20))\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec==2025.2.0 (from -r requirements.txt (line 21))\n",
      "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub==0.28.1 (from -r requirements.txt (line 22))\n",
      "  Using cached huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting idna==3.10 (from -r requirements.txt (line 23))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.32.0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (8.32.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (0.19.2)\n",
      "Collecting Jinja2==3.1.5 (from -r requirements.txt (line 27))\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib==1.4.2 (from -r requirements.txt (line 28))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (5.7.2)\n",
      "Collecting langcodes==3.5.0 (from -r requirements.txt (line 31))\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language_data==1.3.0 (from -r requirements.txt (line 32))\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting marisa-trie==1.2.1 (from -r requirements.txt (line 33))\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 34))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting MarkupSafe==3.0.2 (from -r requirements.txt (line 35))\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (0.1.7)\n",
      "Collecting mdurl==0.1.2 (from -r requirements.txt (line 37))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mpmath==1.3.0 (from -r requirements.txt (line 38))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting murmurhash==1.0.12 (from -r requirements.txt (line 39))\n",
      "  Downloading murmurhash-1.0.12-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (1.6.0)\n",
      "Collecting networkx==3.4.2 (from -r requirements.txt (line 41))\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy==2.2.2 (from -r requirements.txt (line 42))\n",
      "  Downloading numpy-2.2.2-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging==24.2 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (24.2)\n",
      "Requirement already satisfied: parso==0.8.4 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (4.9.0)\n",
      "Collecting pillow==11.1.0 (from -r requirements.txt (line 46))\n",
      "  Downloading pillow-11.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (4.3.6)\n",
      "Collecting preshed==3.0.9 (from -r requirements.txt (line 48))\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.50 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 49)) (3.0.50)\n",
      "Collecting psutil==6.1.1 (from -r requirements.txt (line 50))\n",
      "  Using cached psutil-6.1.1-cp36-abi3-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 51)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 52)) (0.2.3)\n",
      "Collecting pydantic==2.10.6 (from -r requirements.txt (line 53))\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pydantic_core==2.27.2 (from -r requirements.txt (line 54))\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 55)) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 56)) (2.9.0.post0)\n",
      "Collecting PyYAML==6.0.2 (from -r requirements.txt (line 57))\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pyzmq==26.2.1 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 58)) (26.2.1)\n",
      "Collecting qpsolvers==4.4.0 (from -r requirements.txt (line 59))\n",
      "  Using cached qpsolvers-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting regex==2024.11.6 (from -r requirements.txt (line 60))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests==2.32.3 (from -r requirements.txt (line 61))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich==13.9.4 (from -r requirements.txt (line 62))\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting safetensors==0.5.2 (from -r requirements.txt (line 63))\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting scikit-learn==1.6.1 (from -r requirements.txt (line 64))\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy==1.15.1 (from -r requirements.txt (line 65))\n",
      "  Downloading scipy-1.15.1-cp310-cp310-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting sentence-transformers==3.4.1 (from -r requirements.txt (line 66))\n",
      "  Using cached sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting shellingham==1.5.4 (from -r requirements.txt (line 67))\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: six==1.17.0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 68)) (1.17.0)\n",
      "Collecting smart-open==7.1.0 (from -r requirements.txt (line 69))\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting soupsieve==2.6 (from -r requirements.txt (line 70))\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting spacy==3.8.4 (from -r requirements.txt (line 71))\n",
      "  Downloading spacy-3.8.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy==3.0.12 (from -r requirements.txt (line 72))\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers==1.0.5 (from -r requirements.txt (line 73))\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting srsly==2.5.1 (from -r requirements.txt (line 74))\n",
      "  Downloading srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 75)) (0.6.3)\n",
      "Collecting sympy==1.13.1 (from -r requirements.txt (line 76))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting thinc==8.3.4 (from -r requirements.txt (line 77))\n",
      "  Downloading thinc-8.3.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting threadpoolctl==3.5.0 (from -r requirements.txt (line 78))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tokenizers==0.21.0 (from -r requirements.txt (line 79))\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting torch==2.6.0 (from -r requirements.txt (line 80))\n",
      "  Downloading torch-2.6.0-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: tornado==6.4.2 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 81)) (6.4.2)\n",
      "Collecting tqdm==4.67.1 (from -r requirements.txt (line 82))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 83)) (5.14.3)\n",
      "Collecting transformers==4.48.2 (from -r requirements.txt (line 84))\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting typer==0.15.1 (from -r requirements.txt (line 85))\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 86)) (4.12.2)\n",
      "Collecting urllib3==2.3.0 (from -r requirements.txt (line 87))\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting wasabi==1.1.3 (from -r requirements.txt (line 88))\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 89)) (0.2.13)\n",
      "Collecting weasel==0.4.1 (from -r requirements.txt (line 90))\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting wrapt==1.17.2 (from -r requirements.txt (line 91))\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: setuptools in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from marisa-trie==1.2.1->-r requirements.txt (line 33)) (75.6.0)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages (from thinc==8.3.4->-r requirements.txt (line 77)) (1.2.0)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-macosx_10_9_universal2.whl (198 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.1-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.12-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading numpy-2.2.2-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp310-cp310-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
      "Downloading psutil-6.1.1-cp36-abi3-macosx_11_0_arm64.whl (248 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\n",
      "Downloading qpsolvers-4.4.0-py3-none-any.whl (82 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.1-cp310-cp310-macosx_14_0_arm64.whl (24.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading spacy-3.8.4-cp310-cp310-macosx_11_0_arm64.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl (634 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.4/634.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp310-cp310-macosx_11_0_arm64.whl (779 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.4/779.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp310-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: mpmath, es_core_news_sm, cymem, ca_core_news_sm, wrapt, wasabi, urllib3, tqdm, threadpoolctl, sympy, spacy-loggers, spacy-legacy, soupsieve, shellingham, safetensors, regex, PyYAML, pydantic_core, psutil, pillow, numpy, networkx, murmurhash, mdurl, MarkupSafe, marisa-trie, joblib, idna, fsspec, filelock, decorator, cloudpathlib, click, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, scipy, requests, pydantic, preshed, markdown-it-py, language_data, Jinja2, beautifulsoup4, torch, scikit-learn, rich, qpsolvers, langcodes, huggingface-hub, confection, bs4, typer, tokenizers, thinc, weasel, transformers, spacy, sentence-transformers\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 7.0.0\n",
      "    Uninstalling psutil-7.0.0:\n",
      "      Successfully uninstalled psutil-7.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.2.0\n",
      "    Uninstalling decorator-5.2.0:\n",
      "      Successfully uninstalled decorator-5.2.0\n",
      "Successfully installed Jinja2-3.1.5 MarkupSafe-3.0.2 PyYAML-6.0.2 annotated-types-0.7.0 beautifulsoup4-4.13.3 bs4-0.0.2 ca_core_news_sm-3.8.0 catalogue-2.0.10 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 decorator-5.1.1 es_core_news_sm-3.8.0 filelock-3.17.0 fsspec-2025.2.0 huggingface-hub-0.28.1 idna-3.10 joblib-1.4.2 langcodes-3.5.0 language_data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 murmurhash-1.0.12 networkx-3.4.2 numpy-2.2.2 pillow-11.1.0 preshed-3.0.9 psutil-6.1.1 pydantic-2.10.6 pydantic_core-2.27.2 qpsolvers-4.4.0 regex-2024.11.6 requests-2.32.3 rich-13.9.4 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.4.1 shellingham-1.5.4 smart-open-7.1.0 soupsieve-2.6 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 sympy-1.13.1 thinc-8.3.4 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.6.0 tqdm-4.67.1 transformers-4.48.2 typer-0.15.1 urllib3-2.3.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install blis==1.2.0 --ignore-requires-python\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages/qpsolvers/solvers/__init__.py:752: UserWarning: no QP solver found on your system, you can install solvers from PyPI by ``pip install qpsolvers[open_source_solvers]``\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from aima_data.utils import open_data\n",
    "from aima_data.text import words, UnigramWordModel\n",
    "from aima_data.learning import NaiveBayesLearner\n",
    "\n",
    "def recognize_author(text, nbs):\n",
    "   return nbs(words(text.lower()))\n",
    "\n",
    "def create_unigram_model(file_path, n):\n",
    "   text = open_data(file_path).read()\n",
    "   wordseq = words(text)\n",
    "   return UnigramWordModel(wordseq, n)\n",
    "\n",
    "p_miquel = create_unigram_model('CA-Text/bolla_hostel.txt', 5)\n",
    "p_josep = create_unigram_model('CA-Text/tom_sawyer_adventures.txt', 5)\n",
    "\n",
    "nbs = NaiveBayesLearner({('Miquel', 1): p_miquel, ('Josep', 1): p_josep}, simple=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara tenim les dues frases següents:\n",
    "De Miquel dels Sants Oliver:\n",
    "\"La història és un diàleg entre el passat i el present, una constant revisió de criteris i valors.\"\n",
    "De Josep Carner:\n",
    "\"Les paraules són com ocells que volen de branca en branca fins a trobar el seu cant definitiu.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Josep'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_author('ocells que volen de branca en branca fins a trobar el seu cant definitiu.', nbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miquel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_author('entre el passat i el present, una constant revisió de criteris i valors.', nbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generació de text\n",
    "Construïu models n-gram amb n=1, n=3, n=5 i n=7 a partir d'un text triat per vosaltres. Pot ser una obra del Projecte Gutenberg o una notícia de premsa, per exemple. Observau com a mesura que augmenta el nombre de paraules que s'hi han tingut en compte la versemblança del text generat és més gran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram: 1\n",
      "z\n",
      "\n",
      "\n",
      "Ngram: 2\n",
      "plphidyrkeictrzaquctcknmpiglajolm022459121464444845409612720vuzeixtsamlbequrajablmuiobvelplcctsmaunj\n",
      "\n",
      "\n",
      "Ngram: 3\n",
      "yordeudomerlostsigojalrepadricempequincoraromicelionguzmajersecks\n",
      "\n",
      "\n",
      "Ngram: 4\n",
      "neroiconcreo\n",
      "\n",
      "\n",
      "Ngram: 5\n",
      "stemores\n",
      "\n",
      "\n",
      "Ngram: 6\n",
      "ccepting\n",
      "\n",
      "\n",
      "Ngram: 7\n",
      " desnudo\n",
      "\n",
      "\n",
      "Ngram: 8\n",
      " diversos\n",
      "\n",
      "\n",
      "Ngram: 9\n",
      " original\n",
      "\n",
      "\n",
      "Ngram: 10\n",
      " unenforceability\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from aima_data.text import NgramCharModel, words\n",
    "\n",
    "# generate text function\n",
    "def generate_text(model, length, ngram=5):\n",
    "    generate = []\n",
    "    seed = choice(list(model.dictionary.keys()))\n",
    "    generate.extend(seed)\n",
    "\n",
    "    for i in range(length - ngram):\n",
    "        last = tuple(generate[-(ngram - 1):])\n",
    "        next_chars = [j[ngram - 1] for j in model.dictionary.keys() if j[:ngram - 1] == last]\n",
    "\n",
    "        if not next_chars:\n",
    "            break\n",
    "\n",
    "        next_char = choice(next_chars)\n",
    "        generate.append(next_char)\n",
    "\n",
    "    return ''.join(generate)\n",
    "\n",
    "fuenteovejuna = open_data('ES-Text/fuente_ovejuna.txt').read()\n",
    "wordseq = words(fuenteovejuna)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    model = NgramCharModel(i, wordseq)\n",
    "    print(f'Ngram: {i}\\n{generate_text(model=model, length=100, ngram=i)}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anàlisi sintàctica\n",
    "Aplicau les eines d'alguna llibreria Python per analitzar sintàcticament l'oració següent: Tots els éssers humans neixen lliures i iguals en dignitat i drets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_sm')\n",
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ca_core_news_sm\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "es_nlp = spacy.load('es_core_news_sm')\n",
    "ca_nlp = spacy.load('ca_core_news_sm')\n",
    "\n",
    "es_sentence = 'Todos los seres humanos nacen libres e iguales en dignidad y derechos'\n",
    "ca_sentence = 'Tots els éssers humans neixen lliures i iguals en dignitat i drets'\n",
    "\n",
    "es_doc = es_nlp(es_sentence)\n",
    "ca_doc = ca_nlp(ca_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CATALÀ -\n",
      "Noun Phrases: ['Tots els éssers humans', 'dignitat i drets']\n",
      "Verbs: ['neixar']\n",
      "Subjects: ['éssers']\n",
      "Adjectius: ['humans', 'lliures', 'iguals']\n",
      "Preposicions: ['en']\n",
      "Conjuncions: ['i', 'i']\n",
      "+ Informació:\n",
      "Paraula: Tots | Lema: tot | Part del discurs: DET | Dependència sintàctica: det | Head: els\n",
      "Paraula: els | Lema: el | Part del discurs: DET | Dependència sintàctica: det | Head: éssers\n",
      "Paraula: éssers | Lema: ésser | Part del discurs: NOUN | Dependència sintàctica: nsubj | Head: neixen\n",
      "Paraula: humans | Lema: humà | Part del discurs: ADJ | Dependència sintàctica: amod | Head: éssers\n",
      "Paraula: neixen | Lema: neixar | Part del discurs: VERB | Dependència sintàctica: ROOT | Head: neixen\n",
      "Paraula: lliures | Lema: lliure | Part del discurs: ADJ | Dependència sintàctica: obj | Head: neixen\n",
      "Paraula: i | Lema: i | Part del discurs: CCONJ | Dependència sintàctica: cc | Head: iguals\n",
      "Paraula: iguals | Lema: igual | Part del discurs: ADJ | Dependència sintàctica: conj | Head: lliures\n",
      "Paraula: en | Lema: en | Part del discurs: ADP | Dependència sintàctica: case | Head: dignitat\n",
      "Paraula: dignitat | Lema: dignitat | Part del discurs: NOUN | Dependència sintàctica: nmod | Head: lliures\n",
      "Paraula: i | Lema: i | Part del discurs: CCONJ | Dependència sintàctica: cc | Head: drets\n",
      "Paraula: drets | Lema: dret | Part del discurs: NOUN | Dependència sintàctica: conj | Head: dignitat\n"
     ]
    }
   ],
   "source": [
    "print('- CATALÀ -')\n",
    "print('Noun Phrases:', [chunk.text for chunk in ca_doc.noun_chunks])\n",
    "print('Verbs:', [token.lemma_ for token in ca_doc if token.pos_ == 'VERB'])\n",
    "print('Subjects:', [token.text for token in ca_doc if token.dep_ == 'nsubj'])\n",
    "print('Adjectius:', [token.text for token in ca_doc if token.pos_ == 'ADJ'])\n",
    "print('Preposicions:', [token.text for token in ca_doc if token.pos_ == 'ADP'])\n",
    "print('Conjuncions:', [token.text for token in ca_doc if token.pos_ == 'CCONJ'])\n",
    "print('+ Informació:')\n",
    "for token in ca_doc:\n",
    "    print(f'Paraula: {token.text} | Lema: {token.lemma_} | Part del discurs: {token.pos_} | Dependència sintàctica: {token.dep_} | Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CASTELLANO -\n",
      "Noun Phrases: ['Todos los seres humanos', 'dignidad', 'derechos']\n",
      "Verbos: ['nacer']\n",
      "Sujetos: ['seres']\n",
      "Adjetivos: ['humanos', 'libres', 'iguales']\n",
      "Preposiciones: ['en']\n",
      "Conjunciones: ['e', 'y']\n",
      "+ Información:\n",
      "Palabra: Todos | Lema: todo | Parte del discurso: DET | Dependencia sintáctica: det | Head: los\n",
      "Palabra: los | Lema: el | Parte del discurso: DET | Dependencia sintáctica: det | Head: seres\n",
      "Palabra: seres | Lema: ser | Parte del discurso: NOUN | Dependencia sintáctica: nsubj | Head: nacen\n",
      "Palabra: humanos | Lema: humano | Parte del discurso: ADJ | Dependencia sintáctica: amod | Head: seres\n",
      "Palabra: nacen | Lema: nacer | Parte del discurso: VERB | Dependencia sintáctica: ROOT | Head: nacen\n",
      "Palabra: libres | Lema: libre | Parte del discurso: ADJ | Dependencia sintáctica: obj | Head: nacen\n",
      "Palabra: e | Lema: e | Parte del discurso: CCONJ | Dependencia sintáctica: cc | Head: iguales\n",
      "Palabra: iguales | Lema: igual | Parte del discurso: ADJ | Dependencia sintáctica: conj | Head: libres\n",
      "Palabra: en | Lema: en | Parte del discurso: ADP | Dependencia sintáctica: case | Head: dignidad\n",
      "Palabra: dignidad | Lema: dignidad | Parte del discurso: NOUN | Dependencia sintáctica: nmod | Head: libres\n",
      "Palabra: y | Lema: y | Parte del discurso: CCONJ | Dependencia sintáctica: cc | Head: derechos\n",
      "Palabra: derechos | Lema: derecho | Parte del discurso: NOUN | Dependencia sintáctica: conj | Head: libres\n"
     ]
    }
   ],
   "source": [
    "print('- CASTELLANO -')\n",
    "print('Noun Phrases:', [chunk.text for chunk in es_doc.noun_chunks])\n",
    "print('Verbos:', [token.lemma_ for token in es_doc if token.pos_ == 'VERB'])\n",
    "print('Sujetos:', [token.text for token in es_doc if token.dep_ == 'nsubj'])\n",
    "print('Adjetivos:', [token.text for token in es_doc if token.pos_ == 'ADJ'])\n",
    "print('Preposiciones:', [token.text for token in es_doc if token.pos_ == 'ADP'])\n",
    "print('Conjunciones:', [token.text for token in es_doc if token.pos_ == 'CCONJ'])\n",
    "print('+ Información:')\n",
    "for token in es_doc:\n",
    "    print(f'Palabra: {token.text} | Lema: {token.lemma_} | Parte del discurso: {token.pos_} | Dependencia sintáctica: {token.dep_} | Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperació d'informació\n",
    "Carregau l'article de la wikipèdia sobre Europa (https://ca.wikipedia.org/wiki/Europa) en una llista de frases i recuperau-ne la informació següent, cercant les frases més semblants.\n",
    "\n",
    "Quan es va gestar el concepte d'Europa?\n",
    "Quina és l'espècie humana autòctona d'Europa?\n",
    "Quan es varen formar els estats actuals d'Europa?\n",
    "Quin clima té Europa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordiop/Desktop/iedib/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: ¿Cuándo se gestó el concepto de Europa?\n",
      "Best Sentence: Vegeu-ne altres significats a «Europa (desambiguació)».\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question 2: ¿Cuál es la especie humana autóctona de Europa?\n",
      "Best Sentence: Aquesta espècie es trobava ja a Europa quan va arribar l'humà de Cromanyó (Homo sapiens), espècie a què pertany tota la humanitat actual.\n",
      "\n",
      "\n",
      "Question 3: ¿Cuándo se formaron los estados actuales de Europa?\n",
      "Best Sentence: Molts dels estats de l'Europa actual es van formar després de la Primera Guerra Mundial.\n",
      "\n",
      "\n",
      "Question 4: ¿Qué clima tiene Europa?\n",
      "Best Sentence: [Consulta: 14 febrer 2011].\n",
      "\n",
      "↑ «European Climate».\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# fetch wikipedia content function\n",
    "def fetch_wikipedia_content(url):\n",
    "    response = get(url)\n",
    "    html = BeautifulSoup(response.text, 'html.parser')\n",
    "    content = ' '.join([p.text for p in html.find_all([\"p\", \"div\"]) if p.text])\n",
    "    return content\n",
    "\n",
    "# processor text with spacy function\n",
    "def process_text_with_spacy(content, model_name='ca_core_news_sm'):\n",
    "    nlp = spacy.load(model_name)\n",
    "    document = nlp(content)\n",
    "    sentences = [sentence.text for sentence in document.sents]\n",
    "    return sentences\n",
    "\n",
    "# best matching thing\n",
    "def find_best_matching_sentences(questions, sentences, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    questions_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "    sentences_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    best_sentences = []\n",
    "    for i, question in enumerate(questions):\n",
    "        scores = util.pytorch_cos_sim(questions_embeddings[i], sentences_embeddings)\n",
    "        best_sentence_idx = scores.argmax()\n",
    "        best_sentences.append(sentences[best_sentence_idx])\n",
    "    return best_sentences\n",
    "\n",
    "url = 'https://ca.wikipedia.org/wiki/Europa'\n",
    "content = fetch_wikipedia_content(url)\n",
    "sentences = process_text_with_spacy(content)\n",
    "\n",
    "questions = [\n",
    "    '¿Cuándo se gestó el concepto de Europa?',\n",
    "    '¿Cuál es la especie humana autóctona de Europa?',\n",
    "    '¿Cuándo se formaron los estados actuales de Europa?',\n",
    "    '¿Qué clima tiene Europa?'\n",
    "]\n",
    "\n",
    "best_sentences = find_best_matching_sentences(questions, sentences)\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    print(f'Question {i+1}: {question}\\nBest Sentence: {best_sentences[i]}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
